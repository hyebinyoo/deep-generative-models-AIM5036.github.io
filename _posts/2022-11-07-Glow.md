# Glow : Generative Flow with Invertible 1x1 convolutions

- [https://arxiv.org/abs/1807.03039]

# Demo


![Demo 1](/Glow_img/Untitled%2025.png)  ![Demo 2](/Glow_img/Untitled%2032.png)
- [https://openai.com/blog/glow/]


# Abstract

- Flow based generative models 장점:
    - 정확한 log-likelihood의 tractability
    - 정확한 latent variable inference의 tractability
    - training과 synthesis의 parallelizability

- Glow 제안: invertible 1x1 convolution을 사용하는 generative flow model
    - 표준 벤치마크에서 log-likelihood가 크게 향상되었다.
    - 큰 이미지를 효율적이고 사실적인 synthesis와 manipulation을 할 수 있다.

    ![Sample img](/Glow_img/Untitled%2026.png)

- 모델 코드: [https://github.com/openai/glow]

# Introduction
    
- 생성 모델링 분야에서 likelihood-based 방법들과 GAN 에서 엄청난 성능 보여주었다.

- 그중 likelihood 기반 방법은 다음과 같이 세 가지 범주로 나눌 수 있다.

    1. Autoregressive model
        - simple하다는 장점이 있지만 병렬화 제한
    2. Variational Auto Encoder(VAE)
        - log-likelihood의 lower bound를 최적화
        - training과 synthesis를 병렬화할 수 있지만 최적화하기 어려움
    3. Flow-based generative model
        - NICE에서 처음 설명되고 RealNVP에서 확장됨
        

- Flow-based generative models은 이제까지 GAN이나 VAE에 비해 주목받지 못했으나, 논문에서는 아래와 같은 장점들을 근거로 이를 사용했다.

- Flow-based generative model 장점

    1. Exact latent-variable inference and log-likelihood evaluation
        - VAE: 데이터포인트에 해당하는 latent variable을 대략적으로만 추론가능
        - GAN: latent를 추론할 Encoder 없음
        - Reversible generative model: latent variable inference 및 log-likelihood evalution을 approximation 없이 정확하게 수행가능
        - 따라서 데이터의 lower bound 대신 정확한 log likelihood를 최적화할 수 있음

    2. Efficient inference and Efficient synthesis
        - Autoregressive model: 가역적이지만 병렬화하기 어렵고 비효율적 (ex. PixelCNN)
        - Flow-based generative model: 추론과 synthesis 모두에 대해 병렬화하는 데 효율적

    3. Useful latent space for downstream tasks
        - Autoregressive model: hidden layer에 알 수 없는 marginal 분포가 있어 데이터의 유효한 조작을 하기 어려움
        - GAN: encoder가 없고 데이터 분포를 완벽하게 지원하지 않을 수 있으므로 일반적으로 latent space에서 직접 표현할 수 없음
        - Reversible generative model & VAE: 유용한 latent space로 인해 datapoints 간의 interpolation과 meaningful modification이 가능
    
    4. Significant potential for memory savings
        - 가역적이 신경망이므로 gradient 계산 시 constant 메모리 필요합니다.
        - 자세한 내용은 RevNet 참고

        ![RevNet](/Glow_img/Untitled%2024.png)

        - RevNet에 대해 간략하게 설명하자면, reversible block을 이용해 메모리에 activation 을 저장하지 않고 back propagation을 할 수 있는 네트워크이다.

        - 참고: [https://arxiv.org/abs/1707.04585]

# 2. Background: Flow-based Generative Models

## Change of Variable ([이 블로그](https://devkihyun.github.io/study/Flow-based-Generative-Models-1-Normalizing-Flow/) 참조)

---

수학에서 어떠한 변수 또는 다변수로 나타낸 식을 다른 변수 또는 다변수로 바꿔 나타내는 것을 변수 변환(Change of Variable)이라고 함. ([위키백과 참조](https://ko.wikipedia.org/wiki/%EB%B3%80%EC%88%98_%EB%B3%80%ED%99%98))<br></br>

어떠한 랜덤 변수 $x$와 랜덤변수 $z$에 대해 다음과 같은 확률 밀도 함수(Probability Density Function, PDF)가 있다고 가정
$x$~$p(x)$
$z$~ $π(z)$<br></br>

여기서
1. 변수 $z$가 변수 $x$를 잘 표현하는 latent variable이고 $z$의 PDF가 주어진다면, 일대일 매핑함수 $x=f(z)$를 사용해서 새로운 landom variable를 구할 수 있지 않을까?
2. 함수 $f$가 invertible이라고 가정한다면(=역함수가 존재한다면) $z=f^{-1}(x)$도 가능하지 않을까?
라는 가정에서 출발
⇒ 우리가 구할 것 : 알려지지 않은 $x$의 확률 분포 $p(x)$를 구하는 것!<br></br>

확률분포 정의에 대해 먼저 적어보면 확률 밀도 함수의 적분은 1이 됨
$∫p(x)dx = ∫π(z)=1$
⇒ 그러므로 두 PDF $p(x),π(z)$의 적분은 둘 다 1이 되는 것을 알 수 있음<br></br>

위의 정의에 변수 변환(Change of Variable)을 적용하면 다음과 같이 변함 
$∫p(x)dx=∫π(f^{-1}(x))df^{-1}(x)$<br></br>

위 식을 미분하여 한번 더 정리하면
$p(x)=π(z)|\frac{dz}{dx}|=π(f^{-1}(x)|\frac{df^{-1}}{dx}|)=π(f^{-1}(x))|(f^{-1})'(x)|$

이렇게 함으로써 알지 못하는 $p(x)$를 $z$의 PDF $π(x)$로 표현할 수 있게 되었음
위의 식을 조금 더 직관적으로 설명하면 서로 다른 변수 $x,z$의 밀도 함수들간의 관계는 $|(f^{-1})'(x)|$만큼의 비율을 갖는다고 볼 수 있음.
우리는 이러한 식들을 다변수 관점으로 다시 표현하고 싶음. 즉 행렬로 표현할 예정임<br></br>

1. 변수 $x,z$를 vector 표기로 바꾸어 $\mathbf{x}, \mathbf{z}$로 나타냄.
2. 행렬의 미분은 행렬의 형태. 이러한 도함수 행렬을 자코비안 행렬(Jacobian Matrix)라고 함
$\mathbf{z}$~$π(\mathbf{z})$, $\mathbf{x}=f(\mathbf{z})$, $\mathbf{z}=f^{-1}(\mathbf{x})$
$p(\mathbf{x})=π(\mathbf{z})|det\frac{d\mathbf{z}}{d\mathbf{x}}|=π(f^{-1}(\mathbf{x}))|det\frac{df^{-1}}{d\mathbf{x}}|$<br></br>

## Jacobian Matrix and Determinant

---

1. 행렬의 미분 = 도함수 행렬 = 자코비안 행렬의 정의 : 벡터 $\mathbf{x}, \mathbf{y}$에 대한 일차 편미분(∂)을 행렬로 나타낸 것
    
    $\mathbf{J}=\frac{d\mathbf{y}}{d\mathbf{x}} = \begin{bmatrix}\frac{∂y_1}{∂\mathbf{x}}\\.\\.\\.\\\frac{∂y_m}{∂\mathbf{x}} \end{bmatrix}=\begin{bmatrix} \frac{∂y_1}{∂x_1}&&...&&\frac{∂y_1}{∂x_n}\\.&&.&&.\\.&&.&&.\\.&&.&&.\\\frac{∂y_m}{∂x_1}&&...&&\frac{∂y_m}{∂x_n} \end{bmatrix}$
    
    =$n$차원 입력 벡터 $\mathbf{x}$를 $m$차원 출력 벡터 $\mathbf{y}$로 매핑하는 함수가 주어지면 이 함수의 모든 1차 편미분 함수 행렬을 Jacobian Matrix로 간단하게 표현할 수 있음<br></br>
    

2. 행렬식(Determinant) : 정방행렬(n x n matrix)에 스칼라를 대응하는 함수의 하나 = 행렬을 대표하는 하나의 스칼라로 계산한다는 것<br></br>
    
    $det M = det\begin{pmatrix} M_{11}&&M_{12}&&...&&M_{1n}\\M_{11}&&M_{12}&&...&&M_{1n}\\.&&.&&.&&.\\.&&.&&.&&.\\M_{m1}&&M_{m2}&&...&&M_{mn} \end{pmatrix}$ 또는 $|M|= det\begin{vmatrix} M_{11}&&M_{12}&&...&&M_{1n}\\M_{11}&&M_{12}&&...&&M_{1n}\\.&&.&&.&&.\\.&&.&&.&&.\\M_{m1}&&M_{m2}&&...&&M_{mn} \end{vmatrix}$로 표기할 수 있음<br></br>
    
    행렬식의 주요성질
    
    1. $det(1_{n⨉n})=1$
    2. $det(MN)=detMdetN$해
    3. 행렬 $M$이 가역행렬(invertible matrix, 역행렬이 있는 행렬)인 경우, $detM≠0$
    4. 행렬 $M$이 가역행렬 인 경우, $detM^{-1}=(detM)^{-1}$
    5. $det(M^T)=detM$

→ 자코비안 행렬과 행렬식으로 전개한 수식 중 $|det\frac{df^{-1}}{d\mathbf{x}}|$를 해결할 수 있음<br></br>

## 역함수 정리, 가역함수의 자코비안

---

1. 역함수 이론
    
    만약 $y=f(x)$와 $x=f^{-1}(y)$가 있다면
    
    $\frac{df^{-1}}{dy}=\frac{dx}{dy}=(\frac{dy}{dx})^{-1}=(\frac{df(x)}{dx})^{-1}$
    
    ⇒ 역함수의 미분과 함수의 미분은 inverse관계
    ⇒ 따라서 역함수의 자코비안을 함수의 자코비안의 inverse로 표현 가능<br></br>
    
2. 가역함수의 자코비안
    
    가역행렬인 경우 행렬식 특성들을 가짐
    $det(M^{-1})=(det(M))^{-1}$
    
    $det(M)det(M^{-1})=det(M·M^{-1})=det(I)=1$<br></br>

## Normalizing flow

---

위에서 $\mathbf{x}$에 대한 확률밀도 함수 $p(\mathbf{x})$를 latent variable라고 가정한 $\mathbf{z}$를 이용해 추정할 수 있음을 확인
하지만, 좋은 $p(\mathbf{x})$를 추정한다는 것(=density estimation)은 쉽지않음
실제 딥러닝 생성 모델들은 posterior distribution $p(\mathbf{z}|\mathbf{x})$를 비교적 간단한 확률분포로 가정하거나 근사(일반적으로 가우시안 분포가 사용됨)
→ 실제 데이터 분포 $p(\mathbf{x})$는 굉장히 복잡하기 때문에 적어도 latent variable의 확률분포가 단순해야지 back propagation 계산을 조금이라도 더 쉽게 할 수 있기 때문<br></br>

Normarlizing Flow : 실제 데이터의 복잡한 확률 분포를 예측하는 데 있어서 효과적인 방식 중 하나

아이디어 : 앞에서 어떤 확률 분포에 역변환 함수를 적용해서 새로운 확률 분포로 변환할 수 있는 것을 확인
→ Normalizing Flow는 단순한 확률 분포에서부터 일련의 역변환 함수를 적용하여 점차 복잡한 확률 분포로 변환해나감
→ 이런 일련의 변환과 Change of Variable이론을 통해 우리는 단순한 분포로부터 새로운 변수들을 반복해서 대체하고 결과적으로 목표하는 최종 변수의 확률 분포를 얻을 수 있게 됨

![flow-based generatibe models](/Glow_img/Untitled%2034.png)

(출처 : [Flow-based Deep Generatibe Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/))

위의 그림을 수식으로 나타내면

$\mathbf{z}_{i-1}$~$p_{i-1}(\mathbf{z}_{i-1})$
$\mathbf{z}_i=f_i(\mathbf{z}_{i-1}),$  thus $\mathbf{z}_{i-1}=f_i^{-1}(\mathbf{z}_i)$
$p_i(\mathbf{z}_i)=p_{i-1}(f_i^{-1}(\mathbf{z}_i))|det\frac{df_i^{-1}}{d\mathbf{z}_i}|$

→ 수식 위에서 정리했던 수식과 동일 :  $p(\mathbf{x})=π(\mathbf{z})|det\frac{d\mathbf{z}}{d\mathbf{x}}|=π(f^{-1}(\mathbf{x}))|det\frac{df^{-1}}{d\mathbf{x}}|$
달라진 점은 연속된 역변환을 나타내기 위해 $i$번째 변수와 확률 분포 그리고 역변환을 표시한 것

이 수식을 조금 더 정리하면

![수식정리](/Glow_img/Untitled%2035.png)

이를 다시 보기 좋게 정리하면

$logp_i(\mathbf{z}_i)=logp_{i-1}(\mathbf{z}_{i-1})-log|det\frac{df_i}{d\mathbf{z}_{i-1}}|$

마지막에 $log$를 취한 것에 대한 것은 확률 등의 수식에서 곱셈이 나올 때 전체적으로 $log$를 취해주면 대부분은 곱을 합으로 바꿀 수 있음 → 계산이 쉬워짐

위의 내용들을 이용하여 $\mathbf{z}_0$의 확률 분포에서부터 시작해 $K$번의 역변환을 통해 $\mathbf{x}$의 확률분포를 구하는 수식을 정리할 수 있음

$\mathbf{x}=\mathbf{z}_K=f_Kºf_{K-1}º···ºf_1(\mathbf{z}_0)$
![Untitled](/Glow_img/Untitled%2036.png)

위와 같이 $logp(\mathbf{x})$를 정의한 후 우리가 가지고 있는 학습 데이터셋$D$에 대해 NLL(Negative Log-Likelihood)를 만들면
$\mathcal{L}(\mathcal{D})=-\frac{1}{|\mathcal{D}|}\sum_{x∈\mathcal{D}}{logp(\mathbf{x})}$
        

# 3. proposed Generative Flow

- NICE 및 RealNVP flow를 기반으로 하는 새로운 flow 제안
- multi-scale architecture에서 결합된 일련의 flow 단계로 구성됨
    
    ![multi-scale architecture](/Glow_img/Untitled%208.png)
    
     actnorm 단계 → invertible 1x1 convolution 단계 → affine 변환단계로 변환되는 generative flow 제안
    
    이 flow는 (b)와 결합됨
    
    이 architecture는 flow의 깊이 $K$와 level 수 $L$을 가짐
    
    ![table](/Glow_img/Untitled%209.png)
    
    제안된 flow의 세 가지 주요 구성요소 : reverse & log-determinant
    
    $x$ : layer의 입력, $y$ : layer의 출력
    
    $x$와 $y$ 모두 공간 차원 [h x w x c]모양의 tensor
    
    $(i,j)$를 이용하여 공간 index를 tensor $x$ 및 $y$로 나타냄
    
    $NN()$함수는 Resnet 및 RealNVP와 같은 얕은 컨볼루션 신경망과 같은 비선형 매핑
    

## 3.1. Actnorm : scale and bias layer with data dependent initialization

---

batch norm 대체.

- activation 함수의 normaliztion을 의미.
- 이 step에서는 각 채널에 scale과 bias parameter를 이용하여 affine transform을 함
- 이는 batch normalization과 유사하지만 mini-batch size 1에서 동작함
- scale, bias parameter는 학습하는 파라미터이지만 초기화됨
- 그래서 데이터의 첫 mini-batch는 actnorm이후 zero mean unit variance가 됨
- 초기화 후 scale과 bias는 데이터와 무관하게 trainable parameter로 취급됨

---

(데이터 종속 초기화가 있는 스케일 및 bias layer)

- 저자는 심층모델을 훈련할 때 직면하는 문제를 완화하기 위해 배치 정규화의 사용을 제안
    - 그러나 배치 정규화에 의해 추가되는 활성화 노이즈의 분산은 GPU 또는 기타 처리장치(PU)당 미니 배치 크기에 반비례하기 때문에
        
        → PU당 미니 배치 크기가 작을수록 성능이 저하되는 것으로 알려져 있음
        
    - 큰 이미지의 경우 메모리 제약으로 인해 PU당 미니배치 크기 1로 학습
- 이 논문에서 배치 정규화와 유사하게 채널 당 스케일 및 편향 매개변수를 사용하여 활성화의 affine transformation을 수행하는 actnorm layer(활성화 정규화용)을 제안
    
    (배치 정규화와 비슷한 기능을 하게 함)
    
- 이것은 데이터 종속 초기화의 한 형태
- 초기화 후 scale과 bias는 데이터와 독립적인 정규 훈련 가능한 매개변수로 처리됨

## 3.2. Invertible 1  1 convolution

---

suffle 대체

- affine layer의 단점은 1~d 채널이 바뀌지 않는다는 것. 이는 변환을 거쳐도 몇몇 요소가 바뀌지 않는 문제가 생김
- Invertible 1x1 convolution은 이를 해결하기 위한 과정이 되고, 이 논문의 main 기여
- 1x1 convolution은 input channel과 ouput channel 수가 같으면 pernutation 연산의 generalization이다.
    
    → 이 permutation을 학습가능하게 만들어버림
    
- 이 말은 결국 1x1 convolution이 output channelr을 input channelr과 같게만 해주면 1~d를 어떻게 잡을 것이냐를 shuffle 해주면서 channel을 해치지 않게 된다는 것

---

- (Dinh et al)은 채널의 순서를 역저니키는 순열과 동등한 것을 포함하는 flow를 제안
- 이 논문에서는 이 고정된 순열을 (학습된) 가역 1x1 convolution으로 대체할 것을 제안
    - 여기서 가중치 행렬은 무작위 회전 행렬로 초기화
- 동일한 수의 입력 및 출력 채널을 갖는 1x1 convolution은 순열 연산의 일반화

- cxc 가중치 행렬 W가 있는 hxwxc 텐서 h의 가역 1x1 convolution의 log-determinant는 계산하기 쉬움
    
    ![eq 9](/Glow_img/Untitled%2010.png)
    
    det(W)를 계산하거나 미분하는 비용 : $O(c^3)$ → 종종 conv2D(h;W)의 계산비용  $O(hwc_2)$과 비슷
    
    가중지 W를 log-determinant가 0인 randome rotation matrix로 초기화
    
    한 SGD단계 후에 이 값은 0에서 발산하기 시작
    

### LU Decomposition

- det(W)를 계산하는 비용은 LU decomposition에서 W를 직접 매개변수화하여 $O(c^3)$ 에서 $O(c)$로 줄일 수 있음
    
    ![eq 10](/Glow_img/Untitled%2011.png)
    
    여기서 P는 순열행렬, L은 대각선에 1이 있는 하부 삼각행렬, U는 대각선에 0이 있는 상부 삼각 행렬, s는 벡터
    
- log-determinant는 간단하게 하면 다음과 같음
    
    ![eq 11](/Glow_img/Untitled%2012.png)
    
    이 논문에서는 wallclock 계산 시간의 큰 차이를 측정하지는 않았지만, 계산 비용의 차이는 큰 c에 대해 중요해질 것
    
- 이 배개변수화에서 우리는 먼저 random rotation matrix X를 샘플링하여 매개변수를 초기화한 다음 P의 해당 값(고정도니 상태로 유지됨)과 L 및 U 및 s의 해당 초기값(최적화됨)을 계산

## 3.3. Affine Coupling Layers

---

- RealNVP에 나온 affine coupling layer와 동일
    1. 처음 d까지의 차원은 그대로 가져감
    2. d+1~D 차원까지는 scale-and shift를 하는 affine transform을 취함
- 여기서 scale, shift parameter들은 처음 d 차원의 function들을 이용
    
    ![affine transform](/Glow_img/Untitled%2013.png)
    
     아래 그림을 통해 쉽게 back propagation이 가능함을 알 수 있음
    
    ![affine transform2](/Glow_img/Untitled%2014.png)
    
    이 변환은 jacobian determinant를 쉽게 계산할 수 있음. 이를 미분해서 쓰면
    
    ![jacobian matrix](/Glow_img/Untitled%2015.png)
    
    Lower traingular matrix 형태 → traingular matrix(각각의 column이 선형동립)
    
    triangular matrix에서는 determinant가 diagonal element의 곱으로 매우 쉬워짐
    
    ![det-jaco](/Glow_img/Untitled%2016.png)
    
    이처럼 affine coupling layer는 normalizing flow를 구현하기에 완벽해보이지만 하나의 affine coulping layer의 일부 채널은 변하지 않고 유지가 됨(1~d차원)
    
    → 그렇기 때문에 각 layer의 순서를 반대로 하여 모든 입력이 변경될 수 있게 설정 가능
    

---

- 순방향 함수, 역방향 함수 및 log-determinant가 계싼적으로 효율적인 강력한 가역변환은 affine coupling layer에 도입
    - 표1에서 추가 coupling layer는 s=1이고 log-determinant가 0인 특수한 경우

### Zero initialization

- 각 affine coupling layer가 초기에 동일 기능을 수행하도록 각 NN()의 마지막 convolution을 0으로 초기화
    - 이것이 매우 깊은 네트워크 훈련하는 데 도움이 된다는 것을 발견

### Split and concatenation

- (Dinh et al, 2014)에서와 같이 split()함수는 입력 텐서를 채널 차원을 따라 두 개의 절반으로 분할하는 반면, concat 연산은 해당 역 연산을 수행 : 단일 텐서로 연결
- (Dinh et al, 2016)에서는 다른 유형의 분할이 도임 : 바둑판 패턴을 사용하여 공간 차원을 따라
    - 이 작업에서는 전체 아키텍처를 단순화하여 채널 차원을 따라 분할만 수행

### Permutation

- 위의 각 flow 단계에는 충분한 flow 단계 후에 각 차원이 다른 모든 차원에 영향을 미칠 수 있도록 하는 일종의 변수 순열이 선행되어야 함
- (Dinh et al, 2014, 2016)에서 구체적으로 수행된 순열 유형은 추가 coupling layer를 수행하기 전에 단순히 채널(feature)의 순서를 반대로 하는 것과 같음
    - 대안은 (고정된) 임의 순열을 수행하는 것
- 이 논문에서의 invertivle 1x1 convolution은 이러한 순열을 일반화한 것
- 실험에서 이 세 가지 선택을 비교

# Related Work

- Glow (Generative Flow with Invertible 1x1 Convolutions)
    - NICE와 RealNVP를 기반으로 더 발전된 모델
    - Sampling하는데 적은 시간이 걸림 (ex. 256x256 image takes less than 1 second)  

- MAF (Masked Autoregressive Flow for Density Estimation)
    - IAF (Inverse Autoregressive Flow) 기반 모델
    - 병렬화 불가로 인해 비효율적  

- Auto Regressive models
    - (ex) PixelRNN, WaveNet etc..
    - 병렬화 불가 및 고차원 데이터 합성 시 시간이 오래걸림  

- GAN (Generative Adversarial Networks)
    - General lack of latent-space encoder
    - General lack of full support over the data
    - optimization 어려움
    - overfitting 및 generalization 평가 어려움


# Experiment
## Performance

![Table 2](/Glow_img/Untitled%2027.png)

- CIFAR-10, ImageNet 등의 데이터 셋에서 Glow가 Real NVP보다 좋은 성능을 보입니다.
- Table의 가장 마지막 행에는 Real NVP논문에서 나온 PixelRNN의 값입니다.

- Glow의 성능이 PixelRNN에 비해서는 떨어졌지만, Resolution이 커지면서 그 격차가 작아지는 경향을 보입니다.
- 위 Table에 나와있진 않지만 PixelRNN은 sequential하기 때문에 오랜 시간이 걸리고
Glow는 이와 비교해 적은 시간이 걸린다는 점을 고려하면 괜찮은 성능을 보인다고 생각합니다.

![performance](/Glow_img/Untitled%2017.png)

- 1x1 convolution을 reverse, shuffle과 비교한 그래프입니다.
- 1x1 convolution을 사용한 경우, 더 안정적이게 높은 성능을 보이는 것을 확인할 수 있습니다.


## Interpolation & Manipulation

### Interpolation

![Interpolation](/Glow_img/Untitled%2020.png)

- Real image pair 에서 latents를 계산해, Interpolation하면 위의 이미지와 같은 결과를 얻을 수 있습니다.
- 논문에서는 위 결과를 통해, manifold 가 smooth하고 중간단계들의 얼굴이 실제 얼굴처럼 보인다고 주장합니다.  


### Manipulation

![Manipulation](/Glow_img/Untitled%2021.png)

- Manipulation에서도 괜찮은 성능을 보이는 것을 확인할 수 있었습니다.
- 오른쪽 수식과 같이 attribute를 가진 latent vector에서 가지지 않은 latent vector를 빼서 manipulation 방향을 정해 결과를 구했습니다.

- Manipulation 영상을 보면, 특정 속성을 변경했을 때 이미지 전체가 밝아지는 경향을 보이는 등, 완전히 independent 하지는 않지만 괜찮은 성능을 보입니다.


## Temperature & Model depth

### Temperature

![Temperature](/Glow_img/Untitled%2022.png)

- Temperature가 클수록 (즉, 1에 가까울수록) 더 다양한 표현이 가능하지만, 너무 과하게 큰 경우 이상한 이미지가 나오는 경우가 존재합니다.
- 논문에서는 temperature를 0.7로 사용한 model(reduced-temperature model)이 가장 적절하다고 합니다.  


### Model depth

![Model depth](/Glow_img/Untitled%2023.png)

- Model depth 실험에서는 L값이 너무 작은 경우에는 feature들이 잘 안 나오는 것을 확인할 수 있습니다.  


# Future work

### Flow ++
- Flow++는 이전과 비교해 3가지 변경사항이 있습니다.

    1. uniform dequantization -> variational flow-based dequantization 
        - Dequantization을 위해 사용하는 uniform noise는 최적의 training loss와 generalization 효과를 내지 못하므로 Variational flow-based dequantization을 사용했다.
    2. affine coupling flow -> logistic mixture CDF coupling flow
        - 충분히 표현력이 강하지 못한 Affine coupling flow를 logistic mixture CDF coupling flow로 변경했다.
    3. convolutional conditioning networks in coupling layer -> -> self-attention in the conditioning networks of coupling layers 
        - 기존 Convolutional network 보다 더 강력한 self-attention in the conditioning networks로 변경했다.

    ![Flow ++](/Glow_img/Untitled%2028.png)

- 성능적인 측면에서 Flow++은 CIFAR 10, ImageNet 데이터 셋에서 Glow보다 더 높은 성능을 보입니다.  


### FFJORD (Free-form Jacobian of Reversible Dynamics)

- Limited model -> Free form model

    - 이전 모델들은 points 매핑할 때 invertible neural network를 이용해 simple distribution을 complex distribution으로 만들었는데 (즉, function x=f(z)의 함수 f를 복잡하게 만듦.) 이때 Jacobian matrix 계산을 쉽게 하기 위해 모델의 architecture를 제한했습니다.
    - FFJORD에서는 Jacobian으로 부터 free form인 모델을 만들 수 있게 되었습니다. 

    ![Flow ++](/Glow_img/Untitled%2029.png)

    - Jacobian determinants 대신, Jacobian의 trace(대각합)들의 적분 값으로 이를 대체하는 방법을 사용합니다.
    - 따라서 determinants 구할 때보다 더 낮은 time complexity를 갖게 됩니다.

    ![Flow ++](/Glow_img/Untitled%2030.png)

    - MNIST에서는 Glow보다 더 놓은 성능을 보였고, CIFAR10에서는 Glow보다는 좋지 않지만, 유사한 성능을 보였다.  


# Q & A

### Q 
- "1 × 1 convolution with equal number of input and output channels is a generalization of a permutation operation."에서 왜 1x1 conv가 순열의 일반화된 표현인 것인지 모르겠습니다.
### Q 
- "Note that a 1 x 1 convolution with equal number of input and output channels is a generalization of a permutation operation" How are 1 x 1 convolution and permutation operation related to each other?
### Q 
- affine coupling layer의 단점이 x1:d가 바뀌지 않는다는 것인데 이를 Invertible 1X1 convolution로 해결하였다고 하였습니다. 특히 input channel과 output channel수가 같으면 permutation연산의 generalization이 가능하다고 하는데 이 부분에 대한 설명이 가능할까요?

### A 
- 순열: 해당 요소를 시퀀스 또는 선형순서로 배열하거나 이미 정렬된 경우 요소의 재배열
or 정렬된 집합의 선형순서를 변경하는 행위
- 1x1 convolution으로 (input과 output의 수만 맞으면) reordering, shuffle 대신 재배열을 하는 것과 동일하기 때문

---

### Q
- 1X1 invertible convolution를 사용하여 해결하고자 한 정확한 문제가 무엇이며, 어떻게 작동하는지 궁금합니다.

### A
- NICE, RealNVP에서는 reordering으로 1:d까지 바뀌지 않는 문제를 해결했습니다. 
reordering과 비슷하게 하는 1x1 convolution을 사용하면 정량적으로 더 낮은 NLL 점수를 얻을 수 있음.

---

### Q 
- 1x1 convolution에서 input의 channel 수와 output의 channel 수가 다르면 invertible이 깨지나요?

### A 
- 당연히 그럴 것으로 예상됩니다.

---

### Q 
- invertable한 1x1 convolution 을 통해 fixed permutation을 수행을 했다고 하는데 어떻게 되는건지 잘 이해가 되지 않습니다.

### A 
- fixed permutation을 대체한 것입니다. Figure 3에서는 shuffle이 fixed random permutation을 뜻합니다. 
Real NVP에서는 단순히 어떠한 고정된 순열을 순서만 바꾸는 형식으로 진행한 것입니다.

---

### Q 
- 1x1convolution으로 channel masking을 바꾸는데, 여기서 채널은 checkerboard와 완전히 다른 RGB채널을 의미하는 것인가요? 그래서 Z1:d 와 Zd+1:K를 구성하는 방식은 NICE처럼 계속 안 바뀌는 게 맞는 건가요?

### A 
- NICE에서 사용한 것이 이전에 말한 “fixed permutation”이라고 생각합니다.
여기서는 그것을 대체하고자 1x1 conv를 사용한 것. 구성방식은 매번 다름.
(완전히 다른 RGB 채널?)

---

### Q 
- 3.2의 1x1 convolution에서 W matrix가 예를 들면 [[1, 0, 0][0, 0, 1][0, 0, 1]] 형태로 초기화 되었다가 점차 W가 학습되어 나가는 것으로 이해했는데 제가 이해한 것이 맞는지 궁금합니다.

### A 
- 그렇다고 생각합니다. 코드에서 (w-init으로) c x c 행렬로 초기화하고 w-init을 initializer로 삼아 get_variable로 (변수) w 생성

![Flow ++](/Glow_img/Untitled%2033.png)

---

### Q 
- step1 actnorm에서 scale과 bias parameter로 affine transform을 하는데 step3 Affine coupling layers에서 Affine transform을 다시 취하는 것으로 이해했는데 왜 중복으로 진행을 하는 것인가요?

### A 
- affine transform 하는 parameter가 다르다고 이해.

---

### Q 
- actnorm을 사용하였는데 왜 batchnormalization 대신 actnorm을 사용하였나요? batchnorm을 쓸 때보다 어떤 점이 좋아졌고, 얼마나 좋아졌는지 궁금합니다.
### Q 
- 왜 act norm을 사용한 걸까요? 큰 이미지에 batch nomalization을 하고싶어서 그런건가요?

### A 
- batchnorm은 GPU 또는 PU 당 mini-batch 크기에 반비례. (PU당 mini-batch가 작으면 성능도 저하됩니다.)
그런데 큰 이미지에서는 메모리 제약으로 인해 mini-batch로 훈련해야 하기에 성능이 저하됩니다.
그런데 actnorm은 mini-batch size에서도 동작을 잘함.

---

### Q 
- multi-scale architecture에서 squeeze는 데이터를 압축하는 역할인데 이 과정이 필요한 이유가 무엇인가요? 그리고 단지 flow 과정을 중첩하는 것이 아니라 multi-scale architecture로 구성한 이유가 있나요?
### Q 
- Multi-scale architecture에서 squeeze가 하는 역할은 무엇인가요?
### Q 
- Could you explain more in detail the multi-scale architecture in the figure 2.b)

### A 
- RealNVP에서 공간 차원에서 문제가 생기는데, 공간이 너무 큰 경우 계산에 어려움.
이를 피하기 위해 채널 별 마스크를 적용하여 처리하고 coupling layer에 대한 입력차원을 줄이기 위해 압축하는 접근법을 사용합니다.
Multi-scale architecture의 주요 이점은 차원 축소하기 위해 squeeze연산자 사용.

---

### Q 
- LU Decomposition으로 computational cost를 줄이는 것 같은데, 식 11이 어떻게 우항이 되는 거죠?
### Q 
- LU decomposition을 해서 시간 복잡도가 c^3에서 c로 줄어드는 과정이 이해가 잘 되지 않습니다.
### Q 
- Could you explain why the cost of computing det(W) drops from O(c^3) to O(c) when parameterizing W directly in its LU decomposition?

### A 
- Flow based 에서는 기존에는 상부삼각행렬 사용.
c x c 행렬로 초기화해서 determinant 계산이 어려운데, LU 분해하여 그 값을 간단하게 구했더니 s가 나왔다.

---

### Q 
- Glow에서 W에 학습에 따라 W의 det이 0에 가까워지는 경우가 문제가 발생할 수도 있을 것 같은데 이를 해결하기 위한 방법이 무엇인가요?

### A 
- W값은 log det가 0으로 초기화 하고, one SGD step 이후에는 0에서 벗어난다고 한다.
때문에 0으로 가까워지는 경우가 발생하지 않을 것 같다.

---

### Q 
- I don't understand the idea of initializing the convolution weight matrix as a rotation. Is it a rotation for each pixel?

### A 
- initializing 할 때 rotation matrix로 초기화를 했다.
Rotation은 pixel기준으로 하지 않는다.

---

### Q 
- 마지막 convolution을 0으로 초기화하는 것이 왜 identity function의 역할을 수행하는지 궁금합니다.

### A 
- 각 affine coupling layer가 초기에 동일한 기능을 수행하도록 함.

---

### Q 
- 8 page의 reduced-Temperature model에 대해서 간단히 설명해주실 수 있을까요?

### A 
- Temperature를 1에 가까울수록, 다양한 표현이 가능하지만, 이상한 이미지가 나오는 경우가 있다. 
reduced-temperature model은 Temperature를 작게 해 적절한 이미지를 찾은 것입니다. (논문에서는 0.7)
