# Glow : Generative Flow with Invertible 1x1 convolutions

- [https://arxiv.org/abs/1807.03039]

# Demo


![Demo 1](/Glow_img/Untitled%2025.png)  ![Demo 2](/Glow_img/Untitled%2032.png)
- [https://openai.com/blog/glow/]


# Abstract

- Flow based generative models 장점:
    - 정확한 log-likelihood의 tractability
    - 정확한 latent variable inference의 tractability
    - training과 synthesis의 parallelizability

- Glow 제안: invertible 1x1 convolution을 사용하는 generative flow model
    - 표준 벤치마크에서 log-likelihood가 크게 향상되었다.
    - 큰 이미지를 효율적이고 사실적인 synthesis와 manipulation을 할 수 있다.

    ![Sample img](/Glow_img/Untitled%2026.png)

- 모델 코드: [https://github.com/openai/glow]

# Introduction
    
- 생성 모델링 분야에서 likelihood-based 방법들과 GAN 에서 엄청난 성능 보여주었다.

- 그중 likelihood 기반 방법은 다음과 같이 세 가지 범주로 나눌 수 있다.

    1. Autoregressive model
        - simple하다는 장점이 있지만 병렬화 제한
    2. Variational Auto Encoder(VAE)
        - log-likelihood의 lower bound를 최적화
        - training과 synthesis를 병렬화할 수 있지만 최적화하기 어려움
    3. Flow-based generative model
        - NICE에서 처음 설명되고 RealNVP에서 확장됨
        

- Flow-based generative models은 이제까지 GAN이나 VAE에 비해 주목받지 못했으나, 논문에서는 아래와 같은 장점들을 근거로 이를 사용했다.

- Flow-based generative model 장점

    1. Exact latent-variable inference and log-likelihood evaluation
        - VAE: 데이터포인트에 해당하는 latent variable을 대략적으로만 추론가능
        - GAN: latent를 추론할 Encoder 없음
        - Reversible generative model: latent variable inference 및 log-likelihood evalution을 approximation 없이 정확하게 수행가능
        - 따라서 데이터의 lower bound 대신 정확한 log likelihood를 최적화할 수 있음

    2. Efficient inference and Efficient synthesis
        - Autoregressive model: 가역적이지만 병렬화하기 어렵고 비효율적 (ex. PixelCNN)
        - Flow-based generative model: 추론과 synthesis 모두에 대해 병렬화하는 데 효율적

    3. Useful latent space for downstream tasks
        - Autoregressive model: hidden layer에 알 수 없는 marginal 분포가 있어 데이터의 유효한 조작을 하기 어려움
        - GAN: encoder가 없고 데이터 분포를 완벽하게 지원하지 않을 수 있으므로 일반적으로 latent space에서 직접 표현할 수 없음
        - Reversible generative model & VAE: 유용한 latent space로 인해 datapoints 간의 interpolation과 meaningful modification이 가능
    
    4. Significant potential for memory savings
        - 가역적이 신경망이므로 gradient 계산 시 constant 메모리 필요합니다.
        - 자세한 내용은 RevNet 참고

        ![RevNet](/Glow_img/Untitled%2024.png)

        - RevNet에 대해 간략하게 설명하자면, reversible block을 이용해 메모리에 activation 을 저장하지 않고 back propagation을 할 수 있는 네트워크이다.

        - 참고: [https://arxiv.org/abs/1707.04585]

# 2. Background: Flow-based Generative Models

## Change of Variable ([이 블로그](https://devkihyun.github.io/study/Flow-based-Generative-Models-1-Normalizing-Flow/) 참조)

---

수학에서 어떠한 변수 또는 다변수로 나타낸 식을 다른 변수 또는 다변수로 바꿔 나타내는 것을 변수 변환(Change of Variable)이라고 함. ([위키백과 참조](https://ko.wikipedia.org/wiki/%EB%B3%80%EC%88%98_%EB%B3%80%ED%99%98))<br></br>

어떠한 랜덤 변수 $x$와 랜덤변수 $z$에 대해 다음과 같은 확률 밀도 함수(Probability Density Function, PDF)가 있다고 가정
$x$~$p(x)$
$z$~ $π(z)$<br></br>

여기서
1. 변수 $z$가 변수 $x$를 잘 표현하는 latent variable이고 $z$의 PDF가 주어진다면, 일대일 매핑함수 $x=f(z)$를 사용해서 새로운 landom variable를 구할 수 있지 않을까?
2. 함수 $f$가 invertible이라고 가정한다면(=역함수가 존재한다면) $z=f^{-1}(x)$도 가능하지 않을까?
라는 가정에서 출발
⇒ 우리가 구할 것 : 알려지지 않은 $x$의 확률 분포 $p(x)$를 구하는 것!<br></br>

확률분포 정의에 대해 먼저 적어보면 확률 밀도 함수의 적분은 1이 됨
$∫p(x)dx = ∫π(z)=1$
⇒ 그러므로 두 PDF $p(x),π(z)$의 적분은 둘 다 1이 되는 것을 알 수 있음<br></br>

위의 정의에 변수 변환(Change of Variable)을 적용하면 다음과 같이 변함 
$∫p(x)dx=∫π(f^{-1}(x))df^{-1}(x)$<br></br>

위 식을 미분하여 한번 더 정리하면
$p(x)=π(z)|\frac{dz}{dx}|=π(f^{-1}(x)|\frac{df^{-1}}{dx}|)=π(f^{-1}(x))|(f^{-1})'(x)|$

이렇게 함으로써 알지 못하는 $p(x)$를 $z$의 PDF $π(x)$로 표현할 수 있게 되었음
위의 식을 조금 더 직관적으로 설명하면 서로 다른 변수 $x,z$의 밀도 함수들간의 관계는 $|(f^{-1})'(x)|$만큼의 비율을 갖는다고 볼 수 있음.
우리는 이러한 식들을 다변수 관점으로 다시 표현하고 싶음. 즉 행렬로 표현할 예정임<br></br>

1. 변수 $x,z$를 vector 표기로 바꾸어 $\mathbf{x}, \mathbf{z}$로 나타냄.
2. 행렬의 미분은 행렬의 형태. 이러한 도함수 행렬을 자코비안 행렬(Jacobian Matrix)라고 함
$\mathbf{z}$~$π(\mathbf{z})$, $\mathbf{x}=f(\mathbf{z})$, $\mathbf{z}=f^{-1}(\mathbf{x})$
$p(\mathbf{x})=π(\mathbf{z})|det\frac{d\mathbf{z}}{d\mathbf{x}}|=π(f^{-1}(\mathbf{x}))|det\frac{df^{-1}}{d\mathbf{x}}|$<br></br>

## Jacobian Matrix and Determinant

---

1. 행렬의 미분 = 도함수 행렬 = 자코비안 행렬의 정의 : 벡터 $\mathbf{x}, \mathbf{y}$에 대한 일차 편미분(∂)을 행렬로 나타낸 것
    
    $\mathbf{J}=\frac{d\mathbf{y}}{d\mathbf{x}} = \begin{bmatrix}\frac{∂y_1}{∂\mathbf{x}}\\.\\.\\.\\\frac{∂y_m}{∂\mathbf{x}} \end{bmatrix}=\begin{bmatrix} \frac{∂y_1}{∂x_1}&&...&&\frac{∂y_1}{∂x_n}\\.&&.&&.\\.&&.&&.\\.&&.&&.\\\frac{∂y_m}{∂x_1}&&...&&\frac{∂y_m}{∂x_n} \end{bmatrix}$
    
    =$n$차원 입력 벡터 $\mathbf{x}$를 $m$차원 출력 벡터 $\mathbf{y}$로 매핑하는 함수가 주어지면 이 함수의 모든 1차 편미분 함수 행렬을 Jacobian Matrix로 간단하게 표현할 수 있음<br></br>
    

2. 행렬식(Determinant) : 정방행렬(n x n matrix)에 스칼라를 대응하는 함수의 하나 = 행렬을 대표하는 하나의 스칼라로 계산한다는 것<br></br>
    
    $det M = det\begin{pmatrix} M_{11}&&M_{12}&&...&&M_{1n}\\M_{11}&&M_{12}&&...&&M_{1n}\\.&&.&&.&&.\\.&&.&&.&&.\\M_{m1}&&M_{m2}&&...&&M_{mn} \end{pmatrix}$ 또는 $|M|= det\begin{vmatrix} M_{11}&&M_{12}&&...&&M_{1n}\\M_{11}&&M_{12}&&...&&M_{1n}\\.&&.&&.&&.\\.&&.&&.&&.\\M_{m1}&&M_{m2}&&...&&M_{mn} \end{vmatrix}$로 표기할 수 있음<br></br>
    
    행렬식의 주요성질
    
    1. $det(1_{n⨉n})=1$
    2. $det(MN)=detMdetN$해
    3. 행렬 $M$이 가역행렬(invertible matrix, 역행렬이 있는 행렬)인 경우, $detM≠0$
    4. 행렬 $M$이 가역행렬 인 경우, $detM^{-1}=(detM)^{-1}$
    5. $det(M^T)=detM$

→ 자코비안 행렬과 행렬식으로 전개한 수식 중 $|det\frac{df^{-1}}{d\mathbf{x}}|$를 해결할 수 있음<br></br>

## 역함수 정리, 가역함수의 자코비안

---

1. 역함수 이론
    
    만약 $y=f(x)$와 $x=f^{-1}(y)$가 있다면
    
    $\frac{df^{-1}}{dy}=\frac{dx}{dy}=(\frac{dy}{dx})^{-1}=(\frac{df(x)}{dx})^{-1}$
    
    ⇒ 역함수의 미분과 함수의 미분은 inverse관계
    ⇒ 따라서 역함수의 자코비안을 함수의 자코비안의 inverse로 표현 가능<br></br>
    
2. 가역함수의 자코비안
    
    가역행렬인 경우 행렬식 특성들을 가짐
    $det(M^{-1})=(det(M))^{-1}$
    
    $det(M)det(M^{-1})=det(M·M^{-1})=det(I)=1$<br></br>

## Normalizing flow

---

위에서 $\mathbf{x}$에 대한 확률밀도 함수 $p(\mathbf{x})$를 latent variable라고 가정한 $\mathbf{z}$를 이용해 추정할 수 있음을 확인
하지만, 좋은 $p(\mathbf{x})$를 추정한다는 것(=density estimation)은 쉽지않음
실제 딥러닝 생성 모델들은 posterior distribution $p(\mathbf{z}|\mathbf{x})$를 비교적 간단한 확률분포로 가정하거나 근사(일반적으로 가우시안 분포가 사용됨)
→ 실제 데이터 분포 $p(\mathbf{x})$는 굉장히 복잡하기 때문에 적어도 latent variable의 확률분포가 단순해야지 back propagation 계산을 조금이라도 더 쉽게 할 수 있기 때문<br></br>

Normarlizing Flow : 실제 데이터의 복잡한 확률 분포를 예측하는 데 있어서 효과적인 방식 중 하나

아이디어 : 앞에서 어떤 확률 분포에 역변환 함수를 적용해서 새로운 확률 분포로 변환할 수 있는 것을 확인
→ Normalizing Flow는 단순한 확률 분포에서부터 일련의 역변환 함수를 적용하여 점차 복잡한 확률 분포로 변환해나감
→ 이런 일련의 변환과 Change of Variable이론을 통해 우리는 단순한 분포로부터 새로운 변수들을 반복해서 대체하고 결과적으로 목표하는 최종 변수의 확률 분포를 얻을 수 있게 됨

![flow-based generatibe models](/Glow_img/Untitled%2034.png)

(출처 : [Flow-based Deep Generatibe Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/))

위의 그림을 수식으로 나타내면

$\mathbf{z}_{i-1}$~$p_{i-1}(\mathbf{z}_{i-1})$
$\mathbf{z}_i=f_i(\mathbf{z}_{i-1}),$  thus $\mathbf{z}_{i-1}=f_i^{-1}(\mathbf{z}_i)$
$p_i(\mathbf{z}_i)=p_{i-1}(f_i^{-1}(\mathbf{z}_i))|det\frac{df_i^{-1}}{d\mathbf{z}_i}|$

→ 수식 위에서 정리했던 수식과 동일 :  $p(\mathbf{x})=π(\mathbf{z})|det\frac{d\mathbf{z}}{d\mathbf{x}}|=π(f^{-1}(\mathbf{x}))|det\frac{df^{-1}}{d\mathbf{x}}|$
달라진 점은 연속된 역변환을 나타내기 위해 $i$번째 변수와 확률 분포 그리고 역변환을 표시한 것

이 수식을 조금 더 정리하면

![수식정리](/Glow_img/Untitled%2035.png)

이를 다시 보기 좋게 정리하면

$logp_i(\mathbf{z}_i)=logp_{i-1}(\mathbf{z}_{i-1})-log|det\frac{df_i}{d\mathbf{z}_{i-1}}|$

마지막에 $log$를 취한 것에 대한 것은 확률 등의 수식에서 곱셈이 나올 때 전체적으로 $log$를 취해주면 대부분은 곱을 합으로 바꿀 수 있음 → 계산이 쉬워짐

위의 내용들을 이용하여 $\mathbf{z}_0$의 확률 분포에서부터 시작해 $K$번의 역변환을 통해 $\mathbf{x}$의 확률분포를 구하는 수식을 정리할 수 있음

$\mathbf{x}=\mathbf{z}_K=f_Kºf_{K-1}º···ºf_1(\mathbf{z}_0)$
![Untitled](/Glow_img/Untitled%2036.png)

위와 같이 $logp(\mathbf{x})$를 정의한 후 우리가 가지고 있는 학습 데이터셋$D$에 대해 NLL(Negative Log-Likelihood)를 만들면
$\mathcal{L}(\mathcal{D})=-\frac{1}{|\mathcal{D}|}\sum_{x∈\mathcal{D}}{logp(\mathbf{x})}$
        

# 3. proposed Generative Flow

 actnorm 단계 → invertible 1x1 convolution 단계 → affine couplint layer 단계 총 3단계를 거치며 이 flow는 Multi-scale architecture에 결합이 되는 것을 아래 그림에서 확인 가능

![Untitled](/Glow_img/Untitled%208.png)

## 3.1. Actnorm : scale and bias layer with data dependent initialization

---

![Untitled](/Glow_img/Untitled%2037.png)

기존의 RealNVP에서 사용하였던 batchnorm을 대체하여 사용.

이 step에서는 scale과 bias parameter로 affine transform을 진행함

batchnorm은 한 GPU 혹은 PU당 처리하는 minibatch size가 클 수록 효과가 좋은데 큰 이미지에서는 메모리 제약으로 인해 minibatch size를 크게 할 수 없어 minibatch size를 1로 훈련해야함

⇒ minibatch 클 수록 효과 좋은데 큰 이미지에서는 제일 작은 1로밖에 훈련 못해서 성능이 안좋음

actnorm이 이를 극복하여 minibatch size가 1일때도 훈련이 잘 되도록 함. 왜?

첫 번째 minibatch의 mean과 variance로 초기화한 parameter로 normalization을 진행→ 초기화된 이후에는 데이터와 독립된 trainable parameter로 취급되기 때문 

batchnorm이 data에서 연산한 runnint statistics를 통해 normalization을 수행한다면, actnrom은 첫 번째 batch에서 연산산 statistics로 초기화한 파라미터를, 이후에는 독립적인 trainable parameter로 상정하고 normalization을 진행한다는 점에서 차이가 존재

actnorm은 channel-wise normalization을 수행. [$h×w×c$]의 image tensor가 주어지면 channel dimension에서 동작

## 3.2. Invertible 1 × 1 convolution

---

![Untitled](/Glow_img/Untitled%2038.png)

위의 그림을 보면 알 수 있듯이 NICE와 RealNVP는 단순히 reverse ordering하는 형태로 순서를 바꿔주었음. 

Glow의 저자들이 reveser ordering 대신 suffle을 했더니 더 좋은 성능(NLL)이 나왔고, 대신에 1x1 convolution을 진행했더니 그보다 더 나은 성능임을 확인하여 이 논문에서는 1x1 convolution에 대해 설명

조금 더 직관적으로 비교하자면 이전에는

$\begin{bmatrix} 0\\1 \end{bmatrix}$ 혹은 $\begin{bmatrix} 1\\0 \end{bmatrix}$과 같은 단순히 순서를 바꾸는 matrix를 이용하였다면 이 논문에서는

$\begin{bmatrix} cosθ&&sinθ\\-sinθ&&cosθ \end{bmatrix}$와 같은 rotation matrix를 사용, 더 풍부한 표현을 가능하게 했다고 말하고 있음

더 범용적으로 사용가능하기 때문에 “순열의 일반화”라는 표현을 논문에서 여러번 언급

### log-determinant of layer

이러한 random rotation matrix를 weighted matrix $W$로 사용, invertible 1x1 convolution의 log-determinant는 determinant of weight matrix $W$로 귀결되며 다음과 같이 정리됨

![Untitled](/Glow_img/Untitled%2010.png)

처음에는 가중지 $W$를 log-determinant가 0인 randome rotation matrix로 초기화 한 후, 한 SGD단계 후에 이 값은 0에서 발산하기 시작

여기서의 문제는 weight Matrix $W$의 determinant 연산이 $O(c^3)$의 비용이 들어 channel이 커질수록 intractable한 문제가 발생. 그래서 저자는 LU Decompostion을 사용

### LU Decompostion

$det(W)$의 계산 비용을 LU Decompostion을 통해 $O(c^3)$ 에서 $O(c)$로 줄임

![Untitled](/Glow_img/Untitled%2011.png)

여기서 $P$는 단순히 LU 분해를 용이하게 하기 위한 permutation matrix라고 생각하면 되는데, 이때 기존 LU Decompostion 공식에 따르면 $W=PLU$로 분해되어야 맞음

하지만 여기서 저자는 상부삼각행렬인 $U$를 대각성분과 그 나머지로 한 번 더 분리함. 그렇게 하였더니 log-determinant를 다음과 같이 간단하게 계산할 수 있게 됨

![Untitled](/Glow_img/Untitled%2012.png)

LU decompostion을 했을 때와 하지 않았을 때 wall-clock 계산 시간의 큰 차이를 측정하지는 않았지만, 계산 비용의 차이는 큰 $c$에 대해서는 확실히 중요해질 것이라고 말하고 있음

## 3.3. Affine Coupling Layers

---

layer 자체는 RealNVP에서 제안된 layer와 동일함. 

간단하게 설명하고 넘어가자면 1~d까지의 차원은 그대로 가져가고 d+1~D까지의 차원읜 scale and shift를 하는 affine transform을 취하는 형태

![Untitled](/Glow_img/Untitled%2013.png)

아래 그림을 통해 쉽게 back propagation이 가능함을 알 수 있음

![Untitled](/Glow_img/Untitled%2014.png)

layer 자체는 동일하지만 Glow에서는 두 가지 trick을 추가하였음

1. Zero initialization
    
    couplint layer는 affine transform에 활용할 parameter를 NN()을 통해 연산
    
    이 때 NN()이후 추가 convolution을 하나 더 두고 이것의 weight를 0으로 두어 학습 초기에 identify function이 되도록 함
    
    → 이렇게 하면 매우 깊은 네트워크의 학습에 도움을 준다고 저자는 말하고 있음
    
2. permutation
    
    NICE에서는 split 함수로 입력 tensor를 채널차원을 따라 두 개의 절반으로 분할하고, concat 연산은 해당 연산의 역 연산을 수행하여 단일 tensor로 연결하는 방식을 채용
    
    RealNVP에서는 NICE와는 다른 유형의 분할이 도입 되었는데, checkboard pattern을 사용하여 공간 차원을 따라 분할을 함
    
    Glow는 invertible 1x1 convolution을 통해 permutation의 일반화된 표현을 사용하기 때문에 RealNVP와 같은 checkboard pattern 형식의 mask가 큰 의미가 없음
    
    그래서 단순히 channel을 split하고 concat하는 방식을 가져옴(이 부분은 NICE것을 차용했다고 생각하면 됨)
    

# Related Work

- Glow (Generative Flow with Invertible 1x1 Convolutions)
    - NICE와 RealNVP를 기반으로 더 발전된 모델
    - Sampling하는데 적은 시간이 걸림 (ex. 256x256 image takes less than 1 second)  

- MAF (Masked Autoregressive Flow for Density Estimation)
    - IAF (Inverse Autoregressive Flow) 기반 모델
    - 병렬화 불가로 인해 비효율적  

- Auto Regressive models
    - (ex) PixelRNN, WaveNet etc..
    - 병렬화 불가 및 고차원 데이터 합성 시 시간이 오래걸림  

- GAN (Generative Adversarial Networks)
    - General lack of latent-space encoder
    - General lack of full support over the data
    - optimization 어려움
    - overfitting 및 generalization 평가 어려움


# Experiment
## Performance

![Table 2](/Glow_img/Untitled%2027.png)

- CIFAR-10, ImageNet 등의 데이터 셋에서 Glow가 Real NVP보다 좋은 성능을 보입니다.
- Table의 가장 마지막 행에는 Real NVP논문에서 나온 PixelRNN의 값입니다.

- Glow의 성능이 PixelRNN에 비해서는 떨어졌지만, Resolution이 커지면서 그 격차가 작아지는 경향을 보입니다.
- 위 Table에 나와있진 않지만 PixelRNN은 sequential하기 때문에 오랜 시간이 걸리고
Glow는 이와 비교해 적은 시간이 걸린다는 점을 고려하면 괜찮은 성능을 보인다고 생각합니다.

![performance](/Glow_img/Untitled%2017.png)

- 1x1 convolution을 reverse, shuffle과 비교한 그래프입니다.
- 1x1 convolution을 사용한 경우, 더 안정적이게 높은 성능을 보이는 것을 확인할 수 있습니다.


## Interpolation & Manipulation

### Interpolation

![Interpolation](/Glow_img/Untitled%2020.png)

- Real image pair 에서 latents를 계산해, Interpolation하면 위의 이미지와 같은 결과를 얻을 수 있습니다.
- 논문에서는 위 결과를 통해, manifold 가 smooth하고 중간단계들의 얼굴이 실제 얼굴처럼 보인다고 주장합니다.  


### Manipulation

![Manipulation](/Glow_img/Untitled%2021.png)

- Manipulation에서도 괜찮은 성능을 보이는 것을 확인할 수 있었습니다.
- 오른쪽 수식과 같이 attribute를 가진 latent vector에서 가지지 않은 latent vector를 빼서 manipulation 방향을 정해 결과를 구했습니다.

- Manipulation 영상을 보면, 특정 속성을 변경했을 때 이미지 전체가 밝아지는 경향을 보이는 등, 완전히 independent 하지는 않지만 괜찮은 성능을 보입니다.


## Temperature & Model depth

### Temperature

![Temperature](/Glow_img/Untitled%2022.png)

- Temperature가 클수록 (즉, 1에 가까울수록) 더 다양한 표현이 가능하지만, 너무 과하게 큰 경우 이상한 이미지가 나오는 경우가 존재합니다.
- 논문에서는 temperature를 0.7로 사용한 model(reduced-temperature model)이 가장 적절하다고 합니다.  


### Model depth

![Model depth](/Glow_img/Untitled%2023.png)

- Model depth 실험에서는 L값이 너무 작은 경우에는 feature들이 잘 안 나오는 것을 확인할 수 있습니다.  


# Future work

### Flow ++
- Flow++는 이전과 비교해 3가지 변경사항이 있습니다.

    1. uniform dequantization -> variational flow-based dequantization 
        - Dequantization을 위해 사용하는 uniform noise는 최적의 training loss와 generalization 효과를 내지 못하므로 Variational flow-based dequantization을 사용했다.
    2. affine coupling flow -> logistic mixture CDF coupling flow
        - 충분히 표현력이 강하지 못한 Affine coupling flow를 logistic mixture CDF coupling flow로 변경했다.
    3. convolutional conditioning networks in coupling layer -> -> self-attention in the conditioning networks of coupling layers 
        - 기존 Convolutional network 보다 더 강력한 self-attention in the conditioning networks로 변경했다.

    ![Flow ++](/Glow_img/Untitled%2028.png)

- 성능적인 측면에서 Flow++은 CIFAR 10, ImageNet 데이터 셋에서 Glow보다 더 높은 성능을 보입니다.  


### FFJORD (Free-form Jacobian of Reversible Dynamics)

- Limited model -> Free form model

    - 이전 모델들은 points 매핑할 때 invertible neural network를 이용해 simple distribution을 complex distribution으로 만들었는데 (즉, function x=f(z)의 함수 f를 복잡하게 만듦.) 이때 Jacobian matrix 계산을 쉽게 하기 위해 모델의 architecture를 제한했습니다.
    - FFJORD에서는 Jacobian으로 부터 free form인 모델을 만들 수 있게 되었습니다. 

    ![Flow ++](/Glow_img/Untitled%2029.png)

    - Jacobian determinants 대신, Jacobian의 trace(대각합)들의 적분 값으로 이를 대체하는 방법을 사용합니다.
    - 따라서 determinants 구할 때보다 더 낮은 time complexity를 갖게 됩니다.

    ![Flow ++](/Glow_img/Untitled%2030.png)

    - MNIST에서는 Glow보다 더 놓은 성능을 보였고, CIFAR10에서는 Glow보다는 좋지 않지만, 유사한 성능을 보였다.  


# Q & A

### Q 
- "1 × 1 convolution with equal number of input and output channels is a generalization of a permutation operation."에서 왜 1x1 conv가 순열의 일반화된 표현인 것인지 모르겠습니다.
### Q 
- "Note that a 1 x 1 convolution with equal number of input and output channels is a generalization of a permutation operation" How are 1 x 1 convolution and permutation operation related to each other?
### Q 
- affine coupling layer의 단점이 x1:d가 바뀌지 않는다는 것인데 이를 Invertible 1X1 convolution로 해결하였다고 하였습니다. 특히 input channel과 output channel수가 같으면 permutation연산의 generalization이 가능하다고 하는데 이 부분에 대한 설명이 가능할까요?

### A 
- 순열: 해당 요소를 시퀀스 또는 선형순서로 배열하거나 이미 정렬된 경우 요소의 재배열
or 정렬된 집합의 선형순서를 변경하는 행위
- 1x1 convolution으로 (input과 output의 수만 맞으면) reordering, shuffle 대신 재배열을 하는 것과 동일하기 때문

---

### Q
- 1X1 invertible convolution를 사용하여 해결하고자 한 정확한 문제가 무엇이며, 어떻게 작동하는지 궁금합니다.

### A
- NICE, RealNVP에서는 reordering으로 1:d까지 바뀌지 않는 문제를 해결했습니다. 
reordering과 비슷하게 하는 1x1 convolution을 사용하면 정량적으로 더 낮은 NLL 점수를 얻을 수 있음.

---

### Q 
- 1x1 convolution에서 input의 channel 수와 output의 channel 수가 다르면 invertible이 깨지나요?

### A 
- 당연히 그럴 것으로 예상됩니다.

---

### Q 
- invertable한 1x1 convolution 을 통해 fixed permutation을 수행을 했다고 하는데 어떻게 되는건지 잘 이해가 되지 않습니다.

### A 
- fixed permutation을 대체한 것입니다. Figure 3에서는 shuffle이 fixed random permutation을 뜻합니다. 
Real NVP에서는 단순히 어떠한 고정된 순열을 순서만 바꾸는 형식으로 진행한 것입니다.

---

### Q 
- 1x1convolution으로 channel masking을 바꾸는데, 여기서 채널은 checkerboard와 완전히 다른 RGB채널을 의미하는 것인가요? 그래서 Z1:d 와 Zd+1:K를 구성하는 방식은 NICE처럼 계속 안 바뀌는 게 맞는 건가요?

### A 
- NICE에서 사용한 것이 이전에 말한 “fixed permutation”이라고 생각합니다.
여기서는 그것을 대체하고자 1x1 conv를 사용한 것. 구성방식은 매번 다름.
(완전히 다른 RGB 채널?)

---

### Q 
- 3.2의 1x1 convolution에서 W matrix가 예를 들면 [[1, 0, 0][0, 0, 1][0, 0, 1]] 형태로 초기화 되었다가 점차 W가 학습되어 나가는 것으로 이해했는데 제가 이해한 것이 맞는지 궁금합니다.

### A 
- 그렇다고 생각합니다. 코드에서 (w-init으로) c x c 행렬로 초기화하고 w-init을 initializer로 삼아 get_variable로 (변수) w 생성

![Flow ++](/Glow_img/Untitled%2033.png)

---

### Q 
- step1 actnorm에서 scale과 bias parameter로 affine transform을 하는데 step3 Affine coupling layers에서 Affine transform을 다시 취하는 것으로 이해했는데 왜 중복으로 진행을 하는 것인가요?

### A 
- affine transform 하는 parameter가 다르다고 이해.

---

### Q 
- actnorm을 사용하였는데 왜 batchnormalization 대신 actnorm을 사용하였나요? batchnorm을 쓸 때보다 어떤 점이 좋아졌고, 얼마나 좋아졌는지 궁금합니다.
### Q 
- 왜 act norm을 사용한 걸까요? 큰 이미지에 batch nomalization을 하고싶어서 그런건가요?

### A 
- batchnorm은 GPU 또는 PU 당 mini-batch 크기에 반비례. (PU당 mini-batch가 작으면 성능도 저하됩니다.)
그런데 큰 이미지에서는 메모리 제약으로 인해 mini-batch로 훈련해야 하기에 성능이 저하됩니다.
그런데 actnorm은 mini-batch size에서도 동작을 잘함.

---

### Q 
- multi-scale architecture에서 squeeze는 데이터를 압축하는 역할인데 이 과정이 필요한 이유가 무엇인가요? 그리고 단지 flow 과정을 중첩하는 것이 아니라 multi-scale architecture로 구성한 이유가 있나요?
### Q 
- Multi-scale architecture에서 squeeze가 하는 역할은 무엇인가요?
### Q 
- Could you explain more in detail the multi-scale architecture in the figure 2.b)

### A 
- RealNVP에서 공간 차원에서 문제가 생기는데, 공간이 너무 큰 경우 계산에 어려움.
이를 피하기 위해 채널 별 마스크를 적용하여 처리하고 coupling layer에 대한 입력차원을 줄이기 위해 압축하는 접근법을 사용합니다.
Multi-scale architecture의 주요 이점은 차원 축소하기 위해 squeeze연산자 사용.

---

### Q 
- LU Decomposition으로 computational cost를 줄이는 것 같은데, 식 11이 어떻게 우항이 되는 거죠?
### Q 
- LU decomposition을 해서 시간 복잡도가 c^3에서 c로 줄어드는 과정이 이해가 잘 되지 않습니다.
### Q 
- Could you explain why the cost of computing det(W) drops from O(c^3) to O(c) when parameterizing W directly in its LU decomposition?

### A 
- Flow based 에서는 기존에는 상부삼각행렬 사용.
c x c 행렬로 초기화해서 determinant 계산이 어려운데, LU 분해하여 그 값을 간단하게 구했더니 s가 나왔다.

---

### Q 
- Glow에서 W에 학습에 따라 W의 det이 0에 가까워지는 경우가 문제가 발생할 수도 있을 것 같은데 이를 해결하기 위한 방법이 무엇인가요?

### A 
- W값은 log det가 0으로 초기화 하고, one SGD step 이후에는 0에서 벗어난다고 한다.
때문에 0으로 가까워지는 경우가 발생하지 않을 것 같다.

---

### Q 
- I don't understand the idea of initializing the convolution weight matrix as a rotation. Is it a rotation for each pixel?

### A 
- initializing 할 때 rotation matrix로 초기화를 했다.
Rotation은 pixel기준으로 하지 않는다.

---

### Q 
- 마지막 convolution을 0으로 초기화하는 것이 왜 identity function의 역할을 수행하는지 궁금합니다.

### A 
- 각 affine coupling layer가 초기에 동일한 기능을 수행하도록 함.

---

### Q 
- 8 page의 reduced-Temperature model에 대해서 간단히 설명해주실 수 있을까요?

### A 
- Temperature를 1에 가까울수록, 다양한 표현이 가능하지만, 이상한 이미지가 나오는 경우가 있다. 
reduced-temperature model은 Temperature를 작게 해 적절한 이미지를 찾은 것입니다. (논문에서는 0.7)
