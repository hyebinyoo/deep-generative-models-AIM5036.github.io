# Glow

# Abstract

- Flow 기반 생성모델 : 정확한 log-likelihood의 tractability, 정확한 latent variable 추론의 tractability, 훈련 및 합성의 병렬 등으로 매우 매력적임
- 이 논문에서 Glow 제안 : invertible 1x1 convolution을 사용하는 generative flow의 간단한 유형
    - 이 방법은 표준 벤치마크에서 log-likelihood가 크게 향상되었음을 보여줌
- 가장 놀라운 것은 plain log-likelihood 목표에 최적화된 flow 기반 생성모델이 큰 이미지를 효율적으로, 사실적으로 합성하고 조작할 수 있음

# Introduction

- 머신러닝 분야에서 해결되지 않은 두 가지 주요문제
    1. 데이터 효율성 : 소수의 데이터포인트에서 학습하는 능력
    2. 일반화 : task 또는 context의 변경에 대한 robustness
- 예를 들어 AI 시스템은 훈련 분포와 다른 입력이 주어지면 전혀 작동하지 않는 경우가 있음
    - 머신러닝의 주요 분야인 생성모델 목표는 다음을 통해 이러한 한계를 극복
        1. 현실적인 world model을 학습하여 (잠재적으로 agent가 세계와 실제 상호작용하기 전에) world model에서 계획할 수 있도록 함
        2. 인간의 감독이나 라벨링을 거의 또는 전혀 요구하지 않으며 입력의 의미있는 feature를 학습

- 생성 모델링은 일반적으로 joint probability distribution의 형태로 지정되는 고차원 입력 데이터 내의 모든 종속성을 모델링하는 작업과 관련되어 있음
    - 이러한 joint model은 데이터에 존재하는 모든 패턴을 잠재적으로 포착하기 때문에 정확한 생성모델을 적용하려면 끝이 없음
    
- 생성 모델링 분야는 likelihood 기반 방법과 GAN을 사용하여 엄청난 성능 보여줌
- likelihood 기반 방법은 세 가지 범주로 나눌 수 있음
    1. Autoregressive model
        - simple하다는 장점이 있지만 병렬화 제한
    2. VAE
        - log-likelihood의 lower bound를 최적화
        - 훈련과 synthesis를 병렬화할 수 있지만 최적화하기 어려움
    3. Flow
        - NICE에서 처음 설명되고 RealNVP에서 확장됨
        
- Flow 기반 생성모델은 이제까지 GAN이나 VAE에 비해 주목받지 못함
- Flow 기반 생성모델의 장점
    1. 정확한 latent variable 추론 및 log-likielihood 평가
        - VAE에서는 데이터포인트에 해당하는 latent variable의 값을 대략적으로만 추론할 수 있었음
        - GAN에서는 latent를 추론할 Encoder가 없음
        - 가역적 생성모델에서 이는 근사 없이 정확하게 수행할 수 있음
        - 이는 정확한 추론 뿐 아니라 데이터의 lower bound 대신 정확한 log likelihood를 최적화할 수 있음
    2. 효율적인 추론과 효율적인 synthesis
        - PixelCNN과 같은 Autoregressive model도 가역적이지만 얘는 synthesis에서 병렬화하기 어렵고 비효율적
        - Glow 및 RealNVP같은 flow 기반 생성모델은 추론과 synthesis 모두에 대해 병렬화하는 데 효율적
    3. downstream task에 유용한 latent space
        - Autoregressive model의 hidden layer에는 알 수 없는 marginal 분포가 있어 데이터의 유효한 조작을 하기 어려움
        - GAN에서는 Encoder가 없고 데이터 분포를 완벽하게 지원하지 않을 수 있으므로 일반적으로 latent space에서 직접 표현할 수 없음
        - 가역 생성모델 및 VAE의 경우는 아님
    
    4. 상당한 메모리 절약 가능성
    
    - 가역 신경망에서 gradient 계산하려면 RevNet에서 설명한대로 깊이가 선형이 아닌 일정한 메모리 양이 필요
    
- Section 3에서는 Glow 설명
- Section 5에서는 이전 flow와 glow를 정량적으로 비교, 섹션 6에서는 고해상도 데이터셋에 대한 모델의 질적 측면 연구

# Background: Flow-based Generative Models

- $x$ :  알 수 없는 실제 분포 $x$ ~$p^*(x)$를 갖는 고차원 랜덤 벡터
- 독립적이고 동일하게 분포된(i.i.d.) 데이터세트 $D$를 수집하고 매개변수 θ로  모델 $p_θ(x)$를 선택
    - x가 이산데이터인 경우는, log-likelihood의 목표는 다음을 최소화하는 것과 같음
        
        ![eq 1](/Glow_img/Untitled.png)
        
    - x가 연속데이터인 경우는, log-likelihood의 목표는 다음을 최소화하는 것과 같음
        
        ![eq 2](/Glow_img/Untitled%201.png)
        
    
    ![hat x](/Glow_img/Untitled%202.png)
    
    ![c](/Glow_img/Untitled%203.png)
    
    → a는 데이터의 이산화레벨에 의해 결정. M은 x의 차원
    
- (1)과 (2)의 목적은 예상 compression 비용을 nat 또는 bits로 측정
- 최적화는 데이터의 미니배치를 사용하여 확률적 경사 하강법을 통해 수행

- 대부분의 flow 기반 생성모델에서 생성 프로세스는 다음과 같이 정의됨
    
    ![eq 3, 4](/Glow_img/Untitled%204.png)
    
    - $z$ : latent variable
    - $p_θ(z)$ : 구형 가우스 분포와 같이 tractable한 쉬운 밀도를 가짐
    - $g_θ(..)$ : 가역적이며, bijective라고도 불림
    - 주어진 데이터포인트 $x$에 대해 latent variable 추론은 $z=f_θ(x)=g_θ^-1(x)$라고 함
    - 간결함을 위해 $f$와 $g$에서 θ 생략
- $f$($g$에서도)가 일련의 변환으로 구성된 함수에 중점을 둠 : $f=f_1ºf_2º...ºf_K$
    - $x$와 $z$사이의 관계를 나타낸 식
        
        ![eq 5](/Glow_img/Untitled%205.png)
        
    - 이러한 가역 변환 시퀀스는 (normalizing) flow라고도 함
    - (4)의 change of variable되면, 주어진 데이터포인트가 있는 모델의 확률 밀도 함수(pdf)는 다음과 같음
        
        ![eq 6, 7](/Glow_img/Untitled%206.png)
        
        간결하게 $h_0$은 $x$로 정의, $h_K$는 $z$로 정의.
        
        - scalar 값 $log|det(dh_i/dh_i-1)|$은 자코비안 행렬식  $(dh_i/dh_i-1)$의 절댓값의 로그를 취한 것 → log-determinant라고도 불림
            
            이 값은 transform $f_i$에서 $h_i-1$에서 $h_i$로 갈 때의 log density의 변화
            
        - 좀 어려워보이지만 계산하면 놀라울 정도로 간단함
        - 기본 아이디어는 자코비안 행렬 $(dh_i/dh_i-1)$이 삼각 행렬인 transformation을 선택하는 것
            
            이 경우 log-determinant는 간단함
            
            ![eq 8](/Glow_img/Untitled%207.png)
            
            - sum() : 모든 벡터 요소에 대한 합
            - log() : 요소별 log
            - diag() : 자코비안 행렬의 대각선을 취함
        

# 3. proposed Generative Flow

- NICE 및 RealNVP flow를 기반으로 하는 새로운 flow 제안
- multi-scale architecture에서 결합된 일련의 flow 단계로 구성됨
    
    ![multi-scale architecture](/Glow_img/Untitled%208.png)
    
     actnorm 단계 → invertible 1x1 convolution 단계 → affine 변환단계로 변환되는 generative flow 제안
    
    이 flow는 (b)와 결합됨
    
    이 architecture는 flow의 깊이 $K$와 level 수 $L$을 가짐
    
    ![table](/Glow_img/Untitled%209.png)
    
    제안된 flow의 세 가지 주요 구성요소 : reverse & log-determinant
    
    $x$ : layer의 입력, $y$ : layer의 출력
    
    $x$와 $y$ 모두 공간 차원 [h x w x c]모양의 tensor
    
    $(i,j)$를 이용하여 공간 index를 tensor $x$ 및 $y$로 나타냄
    
    $NN()$함수는 Resnet 및 RealNVP와 같은 얕은 컨볼루션 신경망과 같은 비선형 매핑
    

## 3.1. Actnorm : scale and bias layer with data dependent initialization

---

batch norm 대체.

- activation 함수의 normaliztion을 의미.
- 이 step에서는 각 채널에 scale과 bias parameter를 이용하여 affine transform을 함
- 이는 batch normalization과 유사하지만 mini-batch size 1에서 동작함
- scale, bias parameter는 학습하는 파라미터이지만 초기화됨
- 그래서 데이터의 첫 mini-batch는 actnorm이후 zero mean unit variance가 됨
- 초기화 후 scale과 bias는 데이터와 무관하게 trainable parameter로 취급됨

---

(데이터 종속 초기화가 있는 스케일 및 bias layer)

- 저자는 심층모델을 훈련할 때 직면하는 문제를 완화하기 위해 배치 정규화의 사용을 제안
    - 그러나 배치 정규화에 의해 추가되는 활성화 노이즈의 분산은 GPU 또는 기타 처리장치(PU)당 미니 배치 크기에 반비례하기 때문에
        
        → PU당 미니 배치 크기가 작을수록 성능이 저하되는 것으로 알려져 있음
        
    - 큰 이미지의 경우 메모리 제약으로 인해 PU당 미니배치 크기 1로 학습
- 이 논문에서 배치 정규화와 유사하게 채널 당 스케일 및 편향 매개변수를 사용하여 활성화의 affine transformation을 수행하는 actnorm layer(활성화 정규화용)을 제안
    
    (배치 정규화와 비슷한 기능을 하게 함)
    
- 이것은 데이터 종속 초기화의 한 형태
- 초기화 후 scale과 bias는 데이터와 독립적인 정규 훈련 가능한 매개변수로 처리됨

## 3.2. Invertible 1  1 convolution

---

suffle 대체

- affine layer의 단점은 1~d 채널이 바뀌지 않는다는 것. 이는 변환을 거쳐도 몇몇 요소가 바뀌지 않는 문제가 생김
- Invertible 1x1 convolution은 이를 해결하기 위한 과정이 되고, 이 논문의 main 기여
- 1x1 convolution은 input channel과 ouput channel 수가 같으면 pernutation 연산의 generalization이다.
    
    → 이 permutation을 학습가능하게 만들어버림
    
- 이 말은 결국 1x1 convolution이 output channelr을 input channelr과 같게만 해주면 1~d를 어떻게 잡을 것이냐를 shuffle 해주면서 channel을 해치지 않게 된다는 것

---

- (Dinh et al)은 채널의 순서를 역저니키는 순열과 동등한 것을 포함하는 flow를 제안
- 이 논문에서는 이 고정된 순열을 (학습된) 가역 1x1 convolution으로 대체할 것을 제안
    - 여기서 가중치 행렬은 무작위 회전 행렬로 초기화
- 동일한 수의 입력 및 출력 채널을 갖는 1x1 convolution은 순열 연산의 일반화

- cxc 가중치 행렬 W가 있는 hxwxc 텐서 h의 가역 1x1 convolution의 log-determinant는 계산하기 쉬움
    
    ![eq 9](/Glow_img/Untitled%2010.png)
    
    det(W)를 계산하거나 미분하는 비용 : $O(c^3)$ → 종종 conv2D(h;W)의 계산비용  $O(hwc_2)$과 비슷
    
    가중지 W를 log-determinant가 0인 randome rotation matrix로 초기화
    
    한 SGD단계 후에 이 값은 0에서 발산하기 시작
    

### LU Decomposition

- det(W)를 계산하는 비용은 LU decomposition에서 W를 직접 매개변수화하여 $O(c^3)$ 에서 $O(c)$로 줄일 수 있음
    
    ![eq 10](/Glow_img/Untitled%2011.png)
    
    여기서 P는 순열행렬, L은 대각선에 1이 있는 하부 삼각행렬, U는 대각선에 0이 있는 상부 삼각 행렬, s는 벡터
    
- log-determinant는 간단하게 하면 다음과 같음
    
    ![eq 11](/Glow_img/Untitled%2012.png)
    
    이 논문에서는 wallclock 계산 시간의 큰 차이를 측정하지는 않았지만, 계산 비용의 차이는 큰 c에 대해 중요해질 것
    
- 이 배개변수화에서 우리는 먼저 random rotation matrix X를 샘플링하여 매개변수를 초기화한 다음 P의 해당 값(고정도니 상태로 유지됨)과 L 및 U 및 s의 해당 초기값(최적화됨)을 계산

## 3.3. Affine Coupling Layers

---

- RealNVP에 나온 affine coupling layer와 동일
    1. 처음 d까지의 차원은 그대로 가져감
    2. d+1~D 차원까지는 scale-and shift를 하는 affine transform을 취함
- 여기서 scale, shift parameter들은 처음 d 차원의 function들을 이용
    
    ![affine transform](/Glow_img/Untitled%2013.png)
    
     아래 그림을 통해 쉽게 back propagation이 가능함을 알 수 있음
    
    ![affine transform2](/Glow_img/Untitled%2014.png)
    
    이 변환은 jacobian determinant를 쉽게 계산할 수 있음. 이를 미분해서 쓰면
    
    ![jacobian matrix](/Glow_img/Untitled%2015.png)
    
    Lower traingular matrix 형태 → traingular matrix(각각의 column이 선형동립)
    
    triangular matrix에서는 determinant가 diagonal element의 곱으로 매우 쉬워짐
    
    ![det-jaco](/Glow_img/Untitled%2016.png)
    
    이처럼 affine coupling layer는 normalizing flow를 구현하기에 완벽해보이지만 하나의 affine coulping layer의 일부 채널은 변하지 않고 유지가 됨(1~d차원)
    
    → 그렇기 때문에 각 layer의 순서를 반대로 하여 모든 입력이 변경될 수 있게 설정 가능
    

---

- 순방향 함수, 역방향 함수 및 log-determinant가 계싼적으로 효율적인 강력한 가역변환은 affine coupling layer에 도입
    - 표1에서 추가 coupling layer는 s=1이고 log-determinant가 0인 특수한 경우

### Zero initialization

- 각 affine coupling layer가 초기에 동일 기능을 수행하도록 각 NN()의 마지막 convolution을 0으로 초기화
    - 이것이 매우 깊은 네트워크 훈련하는 데 도움이 된다는 것을 발견

### Split and concatenation

- (Dinh et al, 2014)에서와 같이 split()함수는 입력 텐서를 채널 차원을 따라 두 개의 절반으로 분할하는 반면, concat 연산은 해당 역 연산을 수행 : 단일 텐서로 연결
- (Dinh et al, 2016)에서는 다른 유형의 분할이 도임 : 바둑판 패턴을 사용하여 공간 차원을 따라
    - 이 작업에서는 전체 아키텍처를 단순화하여 채널 차원을 따라 분할만 수행

### Permutation

- 위의 각 flow 단계에는 충분한 flow 단계 후에 각 차원이 다른 모든 차원에 영향을 미칠 수 있도록 하는 일종의 변수 순열이 선행되어야 함
- (Dinh et al, 2014, 2016)에서 구체적으로 수행된 순열 유형은 추가 coupling layer를 수행하기 전에 단순히 채널(feature)의 순서를 반대로 하는 것과 같음
    - 대안은 (고정된) 임의 순열을 수행하는 것
- 이 논문에서의 invertivle 1x1 convolution은 이러한 순열을 일반화한 것
- 실험에서 이 세 가지 선택을 비교

